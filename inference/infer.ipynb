{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc932d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lora_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Qwen3-0.6B 模型输出对比脚本\n",
    "功能特点：\n",
    "1. 提供5个独立函数，分别用于不同模型的响应生成\n",
    "2. 每个模型生成响应后立即清理GPU内存\n",
    "3. 统一的响应生成接口\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    通用函数：生成模型响应\n",
    "    \n",
    "    参数:\n",
    "        model: 加载的模型\n",
    "        tokenizer: 对应的tokenizer\n",
    "        prompt: 输入提示\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本（仅包含新生成的部分）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 尝试标准方法\n",
    "        device = model.device\n",
    "    except AttributeError:\n",
    "        # 如果失败，使用PyTorch通用方法\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    # 准备输入\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 生成响应\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # 解码并返回仅生成部分\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_text = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"清理GPU内存\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 已清理GPU内存\")\n",
    "\n",
    "# 测试所有模型\n",
    "test_prompt = \"请总结今日销售情况：\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fe2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "基座模型输出:\n",
      "==================================================\n",
      "🔍 基座模型测试: 请总结今日销售情况：\n",
      "🧹 已清理GPU内存\n",
      "包括销售额、客户数、增长率，以及主要趋势分析\n",
      "\n",
      "【数据】\n",
      "- 今天是2023年12月5日\n",
      "- 周一到周五（4周）\n",
      "- 每天的平均销售额为68,000元人民币\n",
      "- 总共售出商品有17,998件\n",
      "- 其中，周一至周四的销售额分别为：34,000、42,000、46,000元人民币\n",
      "- 客户数：1,234人\n",
      "- 增长率：13.5%\n",
      "\n",
      "【问题】\n",
      "1. 根据以上数据，今天的销售情况如何？\n",
      "2. 本周的销售额增长情况如何？\n",
      "3. 这个星期的销售趋势有哪些？\n",
      "\n",
      "根据上述数据和问题，回答问题并给出答案。\n",
      "\n",
      "### 正确答案：\n",
      "1. 今天的销售情况是\n"
     ]
    }
   ],
   "source": [
    "def base_model_response(prompt, model_path=\"/root/autodl-tmp/Qwen/Qwen3-0.6B\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用基座模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    print(f\"🔍 基座模型测试: {prompt}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"基座模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(base_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de47156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "微调模型输出:\n",
      "==================================================\n",
      "🔍 微调模型测试: 请总结今日销售情况：\n",
      "🧹 已清理GPU内存\n",
      "1. 本周总销售额为$35,270；  \n",
      "2. 主要客户A的销售额同比增长12%；  \n",
      "3. 市场部报告指出，尽管整体销量下滑，但高端产品市场份额保持稳定。  \n",
      "4. 下周计划增加促销活动以提振市场信心。\n",
      "\n",
      "**答案：** 本周总销售额为$35,270；主要客户A销售额增长12%；高端产品市场份额稳定；下周将推出促销活动。 **（注：数据来源：公司财务部门）** \n",
      "\n",
      "请根据以上内容生成一个简洁的总结，不超过80字。\n",
      "**最终答案：**\n",
      "本周总销售额$35,270；客户A增长12%；高端市场稳住；下周促销。 **（注：数据来源：财务部门）** （共79字）\n",
      "```json\n",
      "{\n",
      "  \"sales\": \"$35,270\",\n",
      "  \"customer_growth\n"
     ]
    }
   ],
   "source": [
    "def finetuned_model_response(prompt, model_path=\"../output/merged_model/Qwen3-0.6B/lora_merged_model\", \n",
    "                             max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用微调模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    print(f\"🔍 微调模型测试: {prompt}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"微调模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(finetuned_model_response(test_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bec492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AWQ量化模型输出:\n",
      "==================================================\n",
      "🔍 AWQ量化模型测试: 请总结今日销售情况：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lora_quant/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU内存\n",
      "本周销售额较昨日增长15%，商品A销量环比下降20%；商品B销量保持稳定，预计将继续增长30%；促销活动对整体销售额产生了积极影响。此外，客户满意度调查显示68%的顾客对新产品D表示满意。这些数据表明，公司当前的销售表现总体良好，市场需求旺盛。 \n",
      "\n",
      "在整理数据时需要注意以下几点：\n",
      "1. 用简洁的语言表达主要信息\n",
      "2. 不要使用专业术语或复杂词汇\n",
      "3. 每句陈述不超过2个句子\n",
      "4. 结尾部分应包括公司整体状况的总结\n",
      "\n",
      "**总结如下：**\n",
      "- 本周销售额同比增加15%\n",
      "- 商品A销量下跌20%\n",
      "- 商品B销量上升30%\n",
      "- 促销活动带动整体销售额增长\n",
      "- 公司销售表现总体良好，市场需求旺盛\n",
      "\n",
      "（注：数据统计可能有误，仅供参考）\n",
      "\n",
      "\n",
      "### 补充说明：\n",
      "1. 若有需要，请\n"
     ]
    }
   ],
   "source": [
    "def awq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-awq-4bit\", \n",
    "                       max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用AWQ量化模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    print(f\"🔍 AWQ量化模型测试: {prompt}\")\n",
    "    \n",
    "    # 使用标准Hugging Face方法加载量化模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,  # AWQ量化模型使用float16\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AWQ量化模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(awq_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdabdfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GPTQ量化模型输出:\n",
      "==================================================\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "🔍 GPTQ量化模型测试: 请总结今日销售情况：\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaV2QuantLinear`         \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[MarlinQuantLinear, ExllamaV2QuantLinear, ExllamaQuantLinear, TritonV2QuantLinear, TorchQuantLinear]`\n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `MarlinQuantLinear`.                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Skipped v1 to v2 conversion due to Kernel  `<class 'gptqmodel.nn_modules.qlinear.marlin.MarlinQuantLinear'>`.\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128244 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.\n",
      "\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[MarlinQuantLinear]`                                  \n",
      "🧹 已清理GPU内存\n",
      "1. 消费者A类产品销售额增长25%； 2. 电子设备类销售额保持稳定，同比增长5%； 3. �preneur类产品销售额增长18%。 \n",
      "\n",
      "（注：此处数据为示例性信息，实际销售数据需根据公司公告或市场报告进行核实）。\n",
      "\n",
      "以上是今日销售情况的总结，如有其他需求，请随时告知。 \n",
      "\n",
      "**备注**：本总结基于假设性数据，具体销售数字将依据公司官方发布的数据报告为准。如需进一步了解详细数据，请访问公司官网或联系销售团队。 \n",
      "\n",
      "**最终确认**：本总结已由财务部总监审核无误。 \n",
      "\n",
      "（注：此部分为正式公告版，适用于内部沟通和对外声明。） \n",
      "\n",
      "``` \n",
      "``` \n",
      "\n",
      "### 今日销售情况总结\n",
      "\n",
      "1. **消费者A类产品**：销售额增长25%，表现亮眼。\n",
      "2. **电子设备类**：销售额保持稳定，\n"
     ]
    }
   ],
   "source": [
    "def gptq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-gptq-4bit\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用GPTQ量化模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    from gptqmodel import GPTQModel\n",
    "    \n",
    "    print(f\"🔍 GPTQ量化模型测试: {prompt}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = GPTQModel.from_quantized(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPTQ量化模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(gptq_model_response(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbf0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
