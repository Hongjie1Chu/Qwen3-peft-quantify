{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc932d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lora_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Qwen3-0.6B æ¨¡å‹è¾“å‡ºå¯¹æ¯”è„šæœ¬\n",
    "åŠŸèƒ½ç‰¹ç‚¹ï¼š\n",
    "1. æä¾›5ä¸ªç‹¬ç«‹å‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºä¸åŒæ¨¡å‹çš„å“åº”ç”Ÿæˆ\n",
    "2. æ¯ä¸ªæ¨¡å‹ç”Ÿæˆå“åº”åç«‹å³æ¸…ç†GPUå†…å­˜\n",
    "3. ç»Ÿä¸€çš„å“åº”ç”Ÿæˆæ¥å£\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    é€šç”¨å‡½æ•°ï¼šç”Ÿæˆæ¨¡å‹å“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model: åŠ è½½çš„æ¨¡å‹\n",
    "        tokenizer: å¯¹åº”çš„tokenizer\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼ˆä»…åŒ…å«æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å°è¯•æ ‡å‡†æ–¹æ³•\n",
    "        device = model.device\n",
    "    except AttributeError:\n",
    "        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨PyTorché€šç”¨æ–¹æ³•\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    # å‡†å¤‡è¾“å…¥\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # è§£ç å¹¶è¿”å›ä»…ç”Ÿæˆéƒ¨åˆ†\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_text = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"æ¸…ç†GPUå†…å­˜\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\")\n",
    "\n",
    "# æµ‹è¯•æ‰€æœ‰æ¨¡å‹\n",
    "test_prompt = \"è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fe2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "åŸºåº§æ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "ğŸ” åŸºåº§æ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n",
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "åŒ…æ‹¬é”€å”®é¢ã€å®¢æˆ·æ•°ã€å¢é•¿ç‡ï¼Œä»¥åŠä¸»è¦è¶‹åŠ¿åˆ†æ\n",
      "\n",
      "ã€æ•°æ®ã€‘\n",
      "- ä»Šå¤©æ˜¯2023å¹´12æœˆ5æ—¥\n",
      "- å‘¨ä¸€åˆ°å‘¨äº”ï¼ˆ4å‘¨ï¼‰\n",
      "- æ¯å¤©çš„å¹³å‡é”€å”®é¢ä¸º68,000å…ƒäººæ°‘å¸\n",
      "- æ€»å…±å”®å‡ºå•†å“æœ‰17,998ä»¶\n",
      "- å…¶ä¸­ï¼Œå‘¨ä¸€è‡³å‘¨å››çš„é”€å”®é¢åˆ†åˆ«ä¸ºï¼š34,000ã€42,000ã€46,000å…ƒäººæ°‘å¸\n",
      "- å®¢æˆ·æ•°ï¼š1,234äºº\n",
      "- å¢é•¿ç‡ï¼š13.5%\n",
      "\n",
      "ã€é—®é¢˜ã€‘\n",
      "1. æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œä»Šå¤©çš„é”€å”®æƒ…å†µå¦‚ä½•ï¼Ÿ\n",
      "2. æœ¬å‘¨çš„é”€å”®é¢å¢é•¿æƒ…å†µå¦‚ä½•ï¼Ÿ\n",
      "3. è¿™ä¸ªæ˜ŸæœŸçš„é”€å”®è¶‹åŠ¿æœ‰å“ªäº›ï¼Ÿ\n",
      "\n",
      "æ ¹æ®ä¸Šè¿°æ•°æ®å’Œé—®é¢˜ï¼Œå›ç­”é—®é¢˜å¹¶ç»™å‡ºç­”æ¡ˆã€‚\n",
      "\n",
      "### æ­£ç¡®ç­”æ¡ˆï¼š\n",
      "1. ä»Šå¤©çš„é”€å”®æƒ…å†µæ˜¯\n"
     ]
    }
   ],
   "source": [
    "def base_model_response(prompt, model_path=\"/root/autodl-tmp/Qwen/Qwen3-0.6B\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨åŸºåº§æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” åŸºåº§æ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"åŸºåº§æ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(base_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de47156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "å¾®è°ƒæ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "ğŸ” å¾®è°ƒæ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n",
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "1. æœ¬å‘¨æ€»é”€å”®é¢ä¸º$35,270ï¼›  \n",
      "2. ä¸»è¦å®¢æˆ·Açš„é”€å”®é¢åŒæ¯”å¢é•¿12%ï¼›  \n",
      "3. å¸‚åœºéƒ¨æŠ¥å‘ŠæŒ‡å‡ºï¼Œå°½ç®¡æ•´ä½“é”€é‡ä¸‹æ»‘ï¼Œä½†é«˜ç«¯äº§å“å¸‚åœºä»½é¢ä¿æŒç¨³å®šã€‚  \n",
      "4. ä¸‹å‘¨è®¡åˆ’å¢åŠ ä¿ƒé”€æ´»åŠ¨ä»¥ææŒ¯å¸‚åœºä¿¡å¿ƒã€‚\n",
      "\n",
      "**ç­”æ¡ˆï¼š** æœ¬å‘¨æ€»é”€å”®é¢ä¸º$35,270ï¼›ä¸»è¦å®¢æˆ·Aé”€å”®é¢å¢é•¿12%ï¼›é«˜ç«¯äº§å“å¸‚åœºä»½é¢ç¨³å®šï¼›ä¸‹å‘¨å°†æ¨å‡ºä¿ƒé”€æ´»åŠ¨ã€‚ **ï¼ˆæ³¨ï¼šæ•°æ®æ¥æºï¼šå…¬å¸è´¢åŠ¡éƒ¨é—¨ï¼‰** \n",
      "\n",
      "è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼Œä¸è¶…è¿‡80å­—ã€‚\n",
      "**æœ€ç»ˆç­”æ¡ˆï¼š**\n",
      "æœ¬å‘¨æ€»é”€å”®é¢$35,270ï¼›å®¢æˆ·Aå¢é•¿12%ï¼›é«˜ç«¯å¸‚åœºç¨³ä½ï¼›ä¸‹å‘¨ä¿ƒé”€ã€‚ **ï¼ˆæ³¨ï¼šæ•°æ®æ¥æºï¼šè´¢åŠ¡éƒ¨é—¨ï¼‰** ï¼ˆå…±79å­—ï¼‰\n",
      "```json\n",
      "{\n",
      "  \"sales\": \"$35,270\",\n",
      "  \"customer_growth\n"
     ]
    }
   ],
   "source": [
    "def finetuned_model_response(prompt, model_path=\"../output/merged_model/Qwen3-0.6B/lora_merged_model\", \n",
    "                             max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” å¾®è°ƒæ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"å¾®è°ƒæ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(finetuned_model_response(test_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bec492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AWQé‡åŒ–æ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "ğŸ” AWQé‡åŒ–æ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lora_quant/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "æœ¬å‘¨é”€å”®é¢è¾ƒæ˜¨æ—¥å¢é•¿15%ï¼Œå•†å“Aé”€é‡ç¯æ¯”ä¸‹é™20%ï¼›å•†å“Bé”€é‡ä¿æŒç¨³å®šï¼Œé¢„è®¡å°†ç»§ç»­å¢é•¿30%ï¼›ä¿ƒé”€æ´»åŠ¨å¯¹æ•´ä½“é”€å”®é¢äº§ç”Ÿäº†ç§¯æå½±å“ã€‚æ­¤å¤–ï¼Œå®¢æˆ·æ»¡æ„åº¦è°ƒæŸ¥æ˜¾ç¤º68%çš„é¡¾å®¢å¯¹æ–°äº§å“Dè¡¨ç¤ºæ»¡æ„ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼Œå…¬å¸å½“å‰çš„é”€å”®è¡¨ç°æ€»ä½“è‰¯å¥½ï¼Œå¸‚åœºéœ€æ±‚æ—ºç››ã€‚ \n",
      "\n",
      "åœ¨æ•´ç†æ•°æ®æ—¶éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
      "1. ç”¨ç®€æ´çš„è¯­è¨€è¡¨è¾¾ä¸»è¦ä¿¡æ¯\n",
      "2. ä¸è¦ä½¿ç”¨ä¸“ä¸šæœ¯è¯­æˆ–å¤æ‚è¯æ±‡\n",
      "3. æ¯å¥é™ˆè¿°ä¸è¶…è¿‡2ä¸ªå¥å­\n",
      "4. ç»“å°¾éƒ¨åˆ†åº”åŒ…æ‹¬å…¬å¸æ•´ä½“çŠ¶å†µçš„æ€»ç»“\n",
      "\n",
      "**æ€»ç»“å¦‚ä¸‹ï¼š**\n",
      "- æœ¬å‘¨é”€å”®é¢åŒæ¯”å¢åŠ 15%\n",
      "- å•†å“Aé”€é‡ä¸‹è·Œ20%\n",
      "- å•†å“Bé”€é‡ä¸Šå‡30%\n",
      "- ä¿ƒé”€æ´»åŠ¨å¸¦åŠ¨æ•´ä½“é”€å”®é¢å¢é•¿\n",
      "- å…¬å¸é”€å”®è¡¨ç°æ€»ä½“è‰¯å¥½ï¼Œå¸‚åœºéœ€æ±‚æ—ºç››\n",
      "\n",
      "ï¼ˆæ³¨ï¼šæ•°æ®ç»Ÿè®¡å¯èƒ½æœ‰è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼‰\n",
      "\n",
      "\n",
      "### è¡¥å……è¯´æ˜ï¼š\n",
      "1. è‹¥æœ‰éœ€è¦ï¼Œè¯·\n"
     ]
    }
   ],
   "source": [
    "def awq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-awq-4bit\", \n",
    "                       max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨AWQé‡åŒ–æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    print(f\"ğŸ” AWQé‡åŒ–æ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # ä½¿ç”¨æ ‡å‡†Hugging Faceæ–¹æ³•åŠ è½½é‡åŒ–æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,  # AWQé‡åŒ–æ¨¡å‹ä½¿ç”¨float16\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AWQé‡åŒ–æ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(awq_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdabdfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GPTQé‡åŒ–æ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "ğŸ” GPTQé‡åŒ–æ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaV2QuantLinear`         \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[MarlinQuantLinear, ExllamaV2QuantLinear, ExllamaQuantLinear, TritonV2QuantLinear, TorchQuantLinear]`\n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `MarlinQuantLinear`.                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Skipped v1 to v2 conversion due to Kernel  `<class 'gptqmodel.nn_modules.qlinear.marlin.MarlinQuantLinear'>`.\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128244 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.\n",
      "\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[MarlinQuantLinear]`                                  \n",
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "1. æ¶ˆè´¹è€…Aç±»äº§å“é”€å”®é¢å¢é•¿25%ï¼› 2. ç”µå­è®¾å¤‡ç±»é”€å”®é¢ä¿æŒç¨³å®šï¼ŒåŒæ¯”å¢é•¿5%ï¼› 3. ï¿½preneurç±»äº§å“é”€å”®é¢å¢é•¿18%ã€‚ \n",
      "\n",
      "ï¼ˆæ³¨ï¼šæ­¤å¤„æ•°æ®ä¸ºç¤ºä¾‹æ€§ä¿¡æ¯ï¼Œå®é™…é”€å”®æ•°æ®éœ€æ ¹æ®å…¬å¸å…¬å‘Šæˆ–å¸‚åœºæŠ¥å‘Šè¿›è¡Œæ ¸å®ï¼‰ã€‚\n",
      "\n",
      "ä»¥ä¸Šæ˜¯ä»Šæ—¥é”€å”®æƒ…å†µçš„æ€»ç»“ï¼Œå¦‚æœ‰å…¶ä»–éœ€æ±‚ï¼Œè¯·éšæ—¶å‘ŠçŸ¥ã€‚ \n",
      "\n",
      "**å¤‡æ³¨**ï¼šæœ¬æ€»ç»“åŸºäºå‡è®¾æ€§æ•°æ®ï¼Œå…·ä½“é”€å”®æ•°å­—å°†ä¾æ®å…¬å¸å®˜æ–¹å‘å¸ƒçš„æ•°æ®æŠ¥å‘Šä¸ºå‡†ã€‚å¦‚éœ€è¿›ä¸€æ­¥äº†è§£è¯¦ç»†æ•°æ®ï¼Œè¯·è®¿é—®å…¬å¸å®˜ç½‘æˆ–è”ç³»é”€å”®å›¢é˜Ÿã€‚ \n",
      "\n",
      "**æœ€ç»ˆç¡®è®¤**ï¼šæœ¬æ€»ç»“å·²ç”±è´¢åŠ¡éƒ¨æ€»ç›‘å®¡æ ¸æ— è¯¯ã€‚ \n",
      "\n",
      "ï¼ˆæ³¨ï¼šæ­¤éƒ¨åˆ†ä¸ºæ­£å¼å…¬å‘Šç‰ˆï¼Œé€‚ç”¨äºå†…éƒ¨æ²Ÿé€šå’Œå¯¹å¤–å£°æ˜ã€‚ï¼‰ \n",
      "\n",
      "``` \n",
      "``` \n",
      "\n",
      "### ä»Šæ—¥é”€å”®æƒ…å†µæ€»ç»“\n",
      "\n",
      "1. **æ¶ˆè´¹è€…Aç±»äº§å“**ï¼šé”€å”®é¢å¢é•¿25%ï¼Œè¡¨ç°äº®çœ¼ã€‚\n",
      "2. **ç”µå­è®¾å¤‡ç±»**ï¼šé”€å”®é¢ä¿æŒç¨³å®šï¼Œ\n"
     ]
    }
   ],
   "source": [
    "def gptq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-gptq-4bit\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    from gptqmodel import GPTQModel\n",
    "    \n",
    "    print(f\"ğŸ” GPTQé‡åŒ–æ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = GPTQModel.from_quantized(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPTQé‡åŒ–æ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(gptq_model_response(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbf0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
