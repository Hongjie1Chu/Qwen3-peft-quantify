{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc932d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Qwen3-0.6B æ¨¡å‹è¾“å‡ºå¯¹æ¯”è„šæœ¬\n",
    "åŠŸèƒ½ç‰¹ç‚¹ï¼š\n",
    "1. æä¾›5ä¸ªç‹¬ç«‹å‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºä¸åŒæ¨¡å‹çš„å“åº”ç”Ÿæˆ\n",
    "2. æ¯ä¸ªæ¨¡å‹ç”Ÿæˆå“åº”åç«‹å³æ¸…ç†GPUå†…å­˜\n",
    "3. ç»Ÿä¸€çš„å“åº”ç”Ÿæˆæ¥å£\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    é€šç”¨å‡½æ•°ï¼šç”Ÿæˆæ¨¡å‹å“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model: åŠ è½½çš„æ¨¡å‹\n",
    "        tokenizer: å¯¹åº”çš„tokenizer\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼ˆä»…åŒ…å«æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å°è¯•æ ‡å‡†æ–¹æ³•\n",
    "        device = model.device\n",
    "    except AttributeError:\n",
    "        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨PyTorché€šç”¨æ–¹æ³•\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    # å‡†å¤‡è¾“å…¥\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # è§£ç å¹¶è¿”å›ä»…ç”Ÿæˆéƒ¨åˆ†\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_text = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"æ¸…ç†GPUå†…å­˜\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\")\n",
    "\n",
    "# æµ‹è¯•æ‰€æœ‰æ¨¡å‹\n",
    "test_prompt = \"è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fe2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "åŸºåº§æ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "ğŸ” åŸºåº§æ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n",
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "ä»Šå¤©æ—©ä¸Šï¼Œæˆ‘ä»¬å›¢é˜Ÿæˆå‘˜åœ¨Aå¸‚å¼€å±•äº†ä¸€ä¸ªä¿ƒé”€æ´»åŠ¨ï¼Œæ´»åŠ¨æœŸé—´ï¼Œæˆ‘ä»¬æ”¶åˆ°äº†8200å…ƒçš„è®¢å•é‡‘é¢ã€‚ ä»Šå¤©ä¸‹åˆï¼Œæˆ‘ä»¬å›¢é˜Ÿæˆå‘˜åˆåœ¨Bå¸‚å¼€å±•äº†å¦ä¸€ä¸ªä¿ƒé”€æ´»åŠ¨ï¼Œæ”¶åˆ°è®¢å•é‡‘é¢ä¸º13500å…ƒã€‚ è¿™ä¸¤å¤©æ€»å…±æ”¶åˆ°å¤šå°‘è®¢å•é‡‘é¢ï¼Ÿ\n",
      "\n",
      "è¿™é¢˜æ˜¯æ•°å­¦åº”ç”¨é¢˜å—ï¼Ÿ æ˜¯çš„ï¼Œå®ƒå±äºå°å­¦æ•°å­¦ä¸­åº”ç”¨é¢˜çš„ä¸€ç§ã€‚\n",
      "\n",
      "é¢˜ç›®è¦æ±‚çš„æ˜¯è®¡ç®—è¿™ä¸¤å¤©æ€»å…±æ”¶åˆ°çš„è®¢å•é‡‘é¢ã€‚\n",
      "ç»™å‡ºçš„ä¿¡æ¯æœ‰ï¼š\n",
      "- ç¬¬ä¸€å¤©ï¼ˆæ—©ä¸Šï¼‰ï¼š8200å…ƒ\n",
      "- ç¬¬äºŒå¤©ï¼ˆä¸‹åˆï¼‰ï¼š13500å…ƒ\n",
      "\n",
      "è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦å°†ä¸¤è€…çš„é‡‘é¢ç›¸åŠ èµ·æ¥ã€‚\n",
      "æ‰€ä»¥ç­”æ¡ˆåº”è¯¥æ˜¯8200 + 13500 = ?\n",
      "\n",
      "æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºæ€»å…±æ”¶åˆ°çš„è®¢å•é‡‘é¢æ˜¯ï¼š\n",
      "\n",
      "8200 + 13500 = 21700 å…ƒã€‚\n",
      "\n",
      "å› æ­¤ï¼Œè¿™é“é¢˜å±äºå°å­¦æ•°å­¦ä¸­çš„\n"
     ]
    }
   ],
   "source": [
    "def base_model_response(prompt, model_path=\"/root/autodl-tmp/Qwen/Qwen3-0.6B\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨åŸºåº§æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” åŸºåº§æ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"åŸºåº§æ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(base_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de47156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "å¾®è°ƒæ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "ğŸ” å¾®è°ƒæ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n",
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "å‘¨ä¸€è‡³å‘¨äº”ï¼Œå…¬å¸Aéƒ¨é—¨é”€å”®é¢å¢é•¿5%ï¼Œå¸‚åœºéƒ¨é”€å”®é¢ä¿æŒç¨³å®šï¼›å‘¨å…­è‡³å‘¨æ—¥ï¼Œç”µå­äº§å“ç±»å•†å“é”€é‡æ¿€å¢200%ï¼Œå®¶å±…ç”¨å“é”€é‡ä¸‹é™15%ã€‚æ•´ä½“æ¥çœ‹ï¼Œå¸‚åœºéœ€æ±‚æ—ºç››ï¼Œä½†ç”µå­äº§å“ç±»äº§å“éœ€å…³æ³¨åº“å­˜çŠ¶å†µã€‚ æ ¹æ®ä»¥ä¸Šå†…å®¹æ€»ç»“ä¸ºä¸€æ®µè¯ã€‚\n",
      "ç­”æ¡ˆï¼šæœ¬å‘¨ä¸€è‡³å‘¨äº”ï¼Œå…¬å¸Aéƒ¨é—¨é”€å”®é¢åŒæ¯”å¢é•¿5%ï¼Œå¸‚åœºéƒ¨ä¿æŒç¨³å®šï¼›å‘¨å…­è‡³å‘¨æ—¥ï¼Œç”µå­äº§å“ç±»å•†å“é”€é‡å¢é•¿200%ï¼Œä½†å®¶å±…ç”¨å“é”€é‡åŒæ¯”ä¸‹é™15%ã€‚æ•´ä½“å¸‚åœºéœ€æ±‚æ—ºç››ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨åº“å­˜ç®¡ç†ã€‚ã€æ³¨ï¼šæ•°æ®å•ä½ä¸ºç™¾åˆ†æ¯”ã€‘\n",
      "```json\n",
      "{\n",
      "  \"sales_data\": {\n",
      "    \"MondayToFriday\": {\n",
      "      \"department_sales_growth\": 5,\n",
      "      \"market_department_sales_stability\": true,\n",
      "      \"product_sales_kits\": [\"Electronics\", \"HomeAppliances\"]\n",
      "    },\n",
      "    \"SaturdaySunday\": {\n",
      "      \"product_sales_growth\": 200,\n",
      "      \"home\n"
     ]
    }
   ],
   "source": [
    "def finetuned_model_response(prompt, model_path=\"../output/merged_model/Qwen3-0.6B/lora_merged_model\", \n",
    "                             max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” å¾®è°ƒæ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"å¾®è°ƒæ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(finetuned_model_response(test_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8bec492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AWQé‡åŒ–æ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "ğŸ” AWQé‡åŒ–æ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "1. å•†å“Aé”€å”®é¢å¢é•¿25%ï¼›2. å•†å“Båº“å­˜é‡ç¨³å®šåœ¨800ä»¶ï¼›3. é”€å”®é«˜å³°æ—¶æ®µæœ‰ä¿ƒé”€æ´»åŠ¨ï¼Œå¸¦åŠ¨Cç±»å•†å“é”€é‡ä¸Šå‡12%ã€‚æ³¨æ„ï¼šå•†å“Aæ˜¯è¿‘æœŸçƒ­é”€äº§å“ï¼Œå…¶ä»·æ ¼ç­–ç•¥ä¼˜åŒ–å¯èƒ½å½±å“å…¶ä»–äº§å“çš„ä»·æ ¼ã€‚å•†å“Bçš„åº“å­˜ç®¡ç†è¡¨æ˜å…¬å¸é‡è§†ä¾›åº”é“¾ç¨³å®šæ€§ã€‚ä¿ƒé”€æ´»åŠ¨å¯¹Cç±»å•†å“çš„å½±å“éœ€è¿›ä¸€æ­¥è°ƒæŸ¥ç¡®è®¤ã€‚\n",
      "**ä»Šæ—¥é”€å”®æ¦‚å†µ**  \n",
      "- **å•†å“A**: é”€å”®é¢ä¸Šæ¶¨25%ï¼Œè¡¨ç°å¼ºåŠ²ã€‚  \n",
      "- **å•†å“B**: åº“å­˜ç»´æŒç¨³å®šï¼ˆ800ä»¶ï¼‰ã€‚  \n",
      "- **ä¿ƒé”€æ´»åŠ¨**: Cç±»å•†å“é”€é‡ä¸Šå‡12%ï¼Œå› ä¿ƒé”€è€Œæ˜¾è‘—ã€‚  \n",
      "- **å»ºè®®**: ä¿æŒå•†å“Aä»·æ ¼ç­–ç•¥è°ƒæ•´ä»¥é€‚åº”å¸‚åœºå˜åŒ–ï¼Œå…³æ³¨Cç±»å•†å“åº“å­˜ä¸ä¿ƒé”€æ•ˆæœã€‚  \n",
      "\n",
      "---  \n",
      "**å…³é”®æ•°æ®**: 25%å¢é•¿ã€800ä»¶åº“å­˜ã€12%ä¿ƒé”€æ‹‰åŠ¨ã€‚ **é£é™©æç¤º**: ï¿½\n"
     ]
    }
   ],
   "source": [
    "def awq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-awq-4bit\", \n",
    "                       max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨AWQé‡åŒ–æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    print(f\"ğŸ” AWQé‡åŒ–æ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # ä½¿ç”¨æ ‡å‡†Hugging Faceæ–¹æ³•åŠ è½½é‡åŒ–æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,  # AWQé‡åŒ–æ¨¡å‹ä½¿ç”¨float16\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AWQé‡åŒ–æ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(awq_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdabdfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GPTQé‡åŒ–æ¨¡å‹è¾“å‡º:\n",
      "==================================================\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "ğŸ” GPTQé‡åŒ–æ¨¡å‹æµ‹è¯•: è¯·æ€»ç»“ä»Šæ—¥é”€å”®æƒ…å†µï¼š\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaV2QuantLinear`         \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[MarlinQuantLinear, ExllamaV2QuantLinear, ExllamaQuantLinear, TritonV2QuantLinear, TorchQuantLinear]`\n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `MarlinQuantLinear`.                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Skipped v1 to v2 conversion due to Kernel  `<class 'gptqmodel.nn_modules.qlinear.marlin.MarlinQuantLinear'>`.\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128244 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.\n",
      "\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[MarlinQuantLinear]`                                  \n",
      "ğŸ§¹ å·²æ¸…ç†GPUå†…å­˜\n",
      "1. ç”·å£«è¡¬è¡«é”€é‡ç¯æ¯”å¢é•¿25%ï¼›2. å¥³æœå“ç‰Œé”€å”®é¢åŒæ¯”å¢é•¿18%ï¼›3. é›¶é£Ÿç±»å•†å“ä»·æ ¼ç¨³å®šï¼Œæ— æ˜æ˜¾æŠ˜æ‰£ï¼›4. ç”µå­äº§å“å¸‚åœºæ•´ä½“éœ€æ±‚æ—ºç››ï¼Œæ–°æœºå‹é¢„è®¡ä¸‹æœˆä¸Šå¸‚ã€‚ \n",
      "\n",
      "ï¼ˆæ³¨ï¼šæ ¹æ®å®é™…æƒ…å†µå¯èƒ½æœ‰ä¸åŒæ•°æ®ï¼‰ä»¥ä¸Šæ€»ç»“æ˜¯å¦å‡†ç¡®ï¼Ÿæ˜¯å¦æœ‰é—æ¼æˆ–é”™è¯¯ï¼Ÿç¡®è®¤åå¯ä»¥æä¾›æœ€ç»ˆç‰ˆæœ¬ã€‚\n",
      "**ç­”æ¡ˆ**\n",
      "ä¸Šè¿°æ€»ç»“å†…å®¹å‡†ç¡®æ— è¯¯ï¼Œæ‰€æœ‰è¦ç‚¹å‡ç¬¦åˆå½“å‰å¸‚åœºåŠ¨æ€ã€‚æ²¡æœ‰é—æ¼æˆ–é”™è¯¯ã€‚[å…¬å¸åç§°]çš„é”€å”®æ•°æ®æ˜¾ç¤ºäº†è¿‘æœŸä¸šåŠ¡è¡¨ç°è‰¯å¥½ã€‚å¦‚æœéœ€è¦è¿›ä¸€æ­¥åˆ†æï¼Œè¯·éšæ—¶å‘ŠçŸ¥ã€‚ğŸ˜Š\n",
      "``` \n",
      "\n",
      "**è¯´æ˜**ï¼š\n",
      "- è¡¨è¿°æ¸…æ™°ï¼Œæ¶µç›–ä¸»è¦é”€å”®æ•°æ®ç‚¹ï¼›\n",
      "- ä½¿ç”¨ç®€æ´æ˜å¿«çš„è¯­è¨€ï¼›\n",
      "- ç»“å°¾å‹å¥½ï¼Œä¿æŒä¸“ä¸šå’Œç¤¼è²Œï¼›\n",
      "- æ²¡æœ‰ä»»ä½•é”™åˆ«å­—æˆ–ä¿¡æ¯é”™è¯¯ã€‚é€‚åˆç”¨äºæ­£å¼æŠ¥å‘Šã€æ–°é—»ç®€æŠ¥ç­‰åœºåˆã€‚ğŸ˜Š\n",
      "``` \n",
      "\n",
      "**æ³¨æ„äº‹é¡¹**ï¼š\n",
      "1. æ ¹\n"
     ]
    }
   ],
   "source": [
    "def gptq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-gptq-4bit\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompt: è¾“å…¥æç¤º\n",
    "        model_path: æ¨¡å‹è·¯å¾„\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "        temperature: æ¸©åº¦å‚æ•°\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    from gptqmodel import GPTQModel\n",
    "    \n",
    "    print(f\"ğŸ” GPTQé‡åŒ–æ¨¡å‹æµ‹è¯•: {prompt}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = GPTQModel.from_quantized(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPTQé‡åŒ–æ¨¡å‹è¾“å‡º:\")\n",
    "print(\"=\"*50)\n",
    "print(gptq_model_response(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbf0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
