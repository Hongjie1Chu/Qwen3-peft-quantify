{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc932d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Qwen3-0.6B 模型输出对比脚本\n",
    "功能特点：\n",
    "1. 提供5个独立函数，分别用于不同模型的响应生成\n",
    "2. 每个模型生成响应后立即清理GPU内存\n",
    "3. 统一的响应生成接口\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    通用函数：生成模型响应\n",
    "    \n",
    "    参数:\n",
    "        model: 加载的模型\n",
    "        tokenizer: 对应的tokenizer\n",
    "        prompt: 输入提示\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本（仅包含新生成的部分）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 尝试标准方法\n",
    "        device = model.device\n",
    "    except AttributeError:\n",
    "        # 如果失败，使用PyTorch通用方法\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    # 准备输入\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 生成响应\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # 解码并返回仅生成部分\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_text = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"清理GPU内存\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 已清理GPU内存\")\n",
    "\n",
    "# 测试所有模型\n",
    "test_prompt = \"请总结今日销售情况：\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fe2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "基座模型输出:\n",
      "==================================================\n",
      "🔍 基座模型测试: 请总结今日销售情况：\n",
      "🧹 已清理GPU内存\n",
      "今天早上，我们团队成员在A市开展了一个促销活动，活动期间，我们收到了8200元的订单金额。 今天下午，我们团队成员又在B市开展了另一个促销活动，收到订单金额为13500元。 这两天总共收到多少订单金额？\n",
      "\n",
      "这题是数学应用题吗？ 是的，它属于小学数学中应用题的一种。\n",
      "\n",
      "题目要求的是计算这两天总共收到的订单金额。\n",
      "给出的信息有：\n",
      "- 第一天（早上）：8200元\n",
      "- 第二天（下午）：13500元\n",
      "\n",
      "要解决这个问题，需要将两者的金额相加起来。\n",
      "所以答案应该是8200 + 13500 = ?\n",
      "\n",
      "根据这些信息，我们可以得出总共收到的订单金额是：\n",
      "\n",
      "8200 + 13500 = 21700 元。\n",
      "\n",
      "因此，这道题属于小学数学中的\n"
     ]
    }
   ],
   "source": [
    "def base_model_response(prompt, model_path=\"/root/autodl-tmp/Qwen/Qwen3-0.6B\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用基座模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    print(f\"🔍 基座模型测试: {prompt}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"基座模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(base_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de47156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "微调模型输出:\n",
      "==================================================\n",
      "🔍 微调模型测试: 请总结今日销售情况：\n",
      "🧹 已清理GPU内存\n",
      "周一至周五，公司A部门销售额增长5%，市场部销售额保持稳定；周六至周日，电子产品类商品销量激增200%，家居用品销量下降15%。整体来看，市场需求旺盛，但电子产品类产品需关注库存状况。 根据以上内容总结为一段话。\n",
      "答案：本周一至周五，公司A部门销售额同比增长5%，市场部保持稳定；周六至周日，电子产品类商品销量增长200%，但家居用品销量同比下降15%。整体市场需求旺盛，建议重点关注库存管理。【注：数据单位为百分比】\n",
      "```json\n",
      "{\n",
      "  \"sales_data\": {\n",
      "    \"MondayToFriday\": {\n",
      "      \"department_sales_growth\": 5,\n",
      "      \"market_department_sales_stability\": true,\n",
      "      \"product_sales_kits\": [\"Electronics\", \"HomeAppliances\"]\n",
      "    },\n",
      "    \"SaturdaySunday\": {\n",
      "      \"product_sales_growth\": 200,\n",
      "      \"home\n"
     ]
    }
   ],
   "source": [
    "def finetuned_model_response(prompt, model_path=\"../output/merged_model/Qwen3-0.6B/lora_merged_model\", \n",
    "                             max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用微调模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    print(f\"🔍 微调模型测试: {prompt}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"微调模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(finetuned_model_response(test_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8bec492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AWQ量化模型输出:\n",
      "==================================================\n",
      "🔍 AWQ量化模型测试: 请总结今日销售情况：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/vllm/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 已清理GPU内存\n",
      "1. 商品A销售额增长25%；2. 商品B库存量稳定在800件；3. 销售高峰时段有促销活动，带动C类商品销量上升12%。注意：商品A是近期热销产品，其价格策略优化可能影响其他产品的价格。商品B的库存管理表明公司重视供应链稳定性。促销活动对C类商品的影响需进一步调查确认。\n",
      "**今日销售概况**  \n",
      "- **商品A**: 销售额上涨25%，表现强劲。  \n",
      "- **商品B**: 库存维持稳定（800件）。  \n",
      "- **促销活动**: C类商品销量上升12%，因促销而显著。  \n",
      "- **建议**: 保持商品A价格策略调整以适应市场变化，关注C类商品库存与促销效果。  \n",
      "\n",
      "---  \n",
      "**关键数据**: 25%增长、800件库存、12%促销拉动。 **风险提示**: �\n"
     ]
    }
   ],
   "source": [
    "def awq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-awq-4bit\", \n",
    "                       max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用AWQ量化模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    print(f\"🔍 AWQ量化模型测试: {prompt}\")\n",
    "    \n",
    "    # 使用标准Hugging Face方法加载量化模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,  # AWQ量化模型使用float16\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AWQ量化模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(awq_model_response(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdabdfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GPTQ量化模型输出:\n",
      "==================================================\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "🔍 GPTQ量化模型测试: 请总结今日销售情况：\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaV2QuantLinear`         \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[MarlinQuantLinear, ExllamaV2QuantLinear, ExllamaQuantLinear, TritonV2QuantLinear, TorchQuantLinear]`\n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `MarlinQuantLinear`.                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Skipped v1 to v2 conversion due to Kernel  `<class 'gptqmodel.nn_modules.qlinear.marlin.MarlinQuantLinear'>`.\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128244 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.\n",
      "\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[MarlinQuantLinear]`                                  \n",
      "🧹 已清理GPU内存\n",
      "1. 男士衬衫销量环比增长25%；2. 女服品牌销售额同比增长18%；3. 零食类商品价格稳定，无明显折扣；4. 电子产品市场整体需求旺盛，新机型预计下月上市。 \n",
      "\n",
      "（注：根据实际情况可能有不同数据）以上总结是否准确？是否有遗漏或错误？确认后可以提供最终版本。\n",
      "**答案**\n",
      "上述总结内容准确无误，所有要点均符合当前市场动态。没有遗漏或错误。[公司名称]的销售数据显示了近期业务表现良好。如果需要进一步分析，请随时告知。😊\n",
      "``` \n",
      "\n",
      "**说明**：\n",
      "- 表述清晰，涵盖主要销售数据点；\n",
      "- 使用简洁明快的语言；\n",
      "- 结尾友好，保持专业和礼貌；\n",
      "- 没有任何错别字或信息错误。适合用于正式报告、新闻简报等场合。😊\n",
      "``` \n",
      "\n",
      "**注意事项**：\n",
      "1. 根\n"
     ]
    }
   ],
   "source": [
    "def gptq_model_response(prompt, model_path=\"../output/quantized_model/Qwen3-0.6B-gptq-4bit\", \n",
    "                        max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    使用GPTQ量化模型生成响应\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入提示\n",
    "        model_path: 模型路径\n",
    "        max_new_tokens: 生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_p: 核采样参数\n",
    "    \n",
    "    返回:\n",
    "        生成的响应文本\n",
    "    \"\"\"\n",
    "    from gptqmodel import GPTQModel\n",
    "    \n",
    "    print(f\"🔍 GPTQ量化模型测试: {prompt}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = GPTQModel.from_quantized(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 生成响应\n",
    "    response = generate_response(model, tokenizer, prompt, max_new_tokens, temperature, top_p)\n",
    "    \n",
    "    # 清理\n",
    "    del model, tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPTQ量化模型输出:\")\n",
    "print(\"=\"*50)\n",
    "print(gptq_model_response(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbf0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
