#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Qwen3-0.6B LoRA å¾®è°ƒè®­ç»ƒè„šæœ¬
åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. æ”¯æŒå®Œå…¨è‡ªå®šä¹‰è·¯å¾„é…ç½®
2. åŒæ—¶ä¿å­˜LoRAé€‚é…å™¨å’Œåˆå¹¶åçš„å®Œæ•´æ¨¡å‹
3. æ¸…æ™°çš„è·¯å¾„ç®¡ç†
"""

import os
import torch
from modelscope import snapshot_download
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, TaskType, get_peft_model
from swanlab.integration.transformers import SwanLabCallback


def load_and_preprocess_data(dataset_path, tokenizer):
    """åŠ è½½æ•°æ®é›†å¹¶è¿›è¡Œé¢„å¤„ç†"""
    print(f"ğŸ”„ åŠ è½½æ•°æ®é›†: {dataset_path}...")
    
    # åŠ è½½ JSONL æ ¼å¼æ•°æ®é›†
    dataset = load_dataset("json", data_files=dataset_path, split="train")
    
    # å®šä¹‰é¢„å¤„ç†å‡½æ•°
    def preprocess_function(samples):
        # æ‹¼æ¥è¾“å…¥è¾“å‡ºå¹¶æ·»åŠ  EOS æ ‡è®°
        texts = [
            inp + out + tokenizer.eos_token 
            for inp, out in zip(samples["input"], samples["output"])
        ]
        
        # ç¼–ç æ–‡æœ¬
        tokenized = tokenizer(
            texts,
            padding="longest",
            max_length=12000,
            truncation=True,
            truncation_side="right",
            return_tensors="pt",
            return_attention_mask=True
        )
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆä»…è®¡ç®—è¾“å‡ºéƒ¨åˆ†çš„æŸå¤±ï¼‰
        input_lens = [
            len(tokenizer.encode(inp, add_special_tokens=False)) 
            for inp in samples["input"]
        ]
        labels = tokenized["input_ids"].clone()
        
        for i, input_len in enumerate(input_lens):
            # å¿½ç•¥è¾“å…¥éƒ¨åˆ†çš„æŸå¤±è®¡ç®—
            labels[i, :input_len] = -100
            
            # ç¡®ä¿ EOS æ ‡è®°å‚ä¸æŸå¤±è®¡ç®—
            if tokenized["input_ids"][i, -1] == tokenizer.eos_token_id:
                labels[i, -1] = tokenizer.eos_token_id
        
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        }
    
    # åº”ç”¨é¢„å¤„ç†
    tokenized_dataset = dataset.map(
        preprocess_function,
        batched=True,
        batch_size=8,
        remove_columns=["input", "output"],
        num_proc=4  # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    )
    
    print(f"âœ… æ•°æ®é›†å¤„ç†å®Œæˆï¼Œæ ·æœ¬æ•°: {len(tokenized_dataset)}")
    return tokenized_dataset


def setup_lora_model(base_model_path):
    """é…ç½® LoRA å¾®è°ƒå‚æ•°"""
    print(f"ğŸ”§ é…ç½® LoRA å¾®è°ƒæ¨¡å‹: {base_model_path}...")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    model.enable_input_require_grads()  # æ¢¯åº¦æ£€æŸ¥ç‚¹å¿…éœ€
    
    # é…ç½® LoRA
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1
    )
    
    # åº”ç”¨ LoRA
    model = get_peft_model(model, config)
    print("âœ… LoRA æ¨¡å‹é…ç½®å®Œæˆ")
    return model


def setup_training(model, tokenizer, tokenized_dataset, output_dir):
    """è®¾ç½®è®­ç»ƒå‚æ•°å¹¶åˆå§‹åŒ–è®­ç»ƒå™¨"""
    print(f"âš™ï¸ é…ç½®è®­ç»ƒå‚æ•° (è¾“å‡ºç›®å½•: {output_dir})...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # è®­ç»ƒå‚æ•°
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=1,
        logging_steps=5,
        num_train_epochs=5,
        save_steps=50,
        learning_rate=1e-4,
        save_on_each_node=True,
        gradient_checkpointing=True,
        report_to="none",
        fp16=True,  # å¯ç”¨åŠç²¾åº¦è®­ç»ƒ
        optim="adamw_torch_fused",  # ä½¿ç”¨ä¼˜åŒ–ç‰ˆä¼˜åŒ–å™¨
        ddp_find_unused_parameters=False
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        padding=True,
        pad_to_multiple_of=8,
        return_tensors="pt"
    )
    
    # åˆå§‹åŒ– SwanLab ç›‘æ§
    swanlab_callback = SwanLabCallback(
        project="Qwen3-0.6B",
        experiment_name="Qwen3-0.6B-Lora"
    )
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        callbacks=[swanlab_callback]
    )
    print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    return trainer


def main():
    """ä¸»è®­ç»ƒæµç¨‹ - æ‰€æœ‰è·¯å¾„é…ç½®åœ¨æ­¤å¤„è‡ªå®šä¹‰"""
    print("âœ¨ å¼€å§‹é…ç½®è®­ç»ƒå‚æ•°...")
    
    # ======================
    # ç”¨æˆ·è‡ªå®šä¹‰è·¯å¾„é…ç½®åŒº
    # ======================
    
    # åŸºç¡€æ¨¡å‹é…ç½®
    BASE_MODEL_SOURCE = "Qwen/Qwen3-0.6B"  # ModelScope æ¨¡å‹æ ‡è¯†æˆ–æœ¬åœ°è·¯å¾„
    BASE_MODEL_NAME = BASE_MODEL_SOURCE.split('/')[-1]
    MODEL_CACHE_DIR = "/root/autodl-tmp/"   # æ¨¡å‹ç¼“å­˜ç›®å½•
    
    # æ•°æ®é›†é…ç½®
    DATASET_PATH = "../../datasets/peft_data.jsonl"
    
    # è¾“å‡ºç›®å½•é…ç½®
    OUTPUT_PATH = "../../output"
    TRAINING_OUTPUT_DIR = f"{OUTPUT_PATH}/training_checkpoints/{BASE_MODEL_NAME}/lora_checkpoint"  # è®­ç»ƒæ£€æŸ¥ç‚¹ç›®å½•
    LORA_ADAPTER_DIR = f"{OUTPUT_PATH}/peft_model/{BASE_MODEL_NAME}/lora_adapter"               # LoRAé€‚é…å™¨ä¿å­˜ç›®å½•
    MERGED_MODEL_DIR = f"{OUTPUT_PATH}/merged_model/{BASE_MODEL_NAME}/lora_merged_model"               # åˆå¹¶æ¨¡å‹ä¿å­˜ç›®å½•
    
    # ======================
    # è·¯å¾„é…ç½®ç»“æŸ
    # ======================
    
    print("ğŸ“Œ è‡ªå®šä¹‰è·¯å¾„é…ç½®:")
    print(f"  â€¢ åŸºç¡€æ¨¡å‹æ¥æº: {BASE_MODEL_SOURCE}")
    print(f"  â€¢ æ¨¡å‹ç¼“å­˜ç›®å½•: {MODEL_CACHE_DIR}")
    print(f"  â€¢ æ•°æ®é›†è·¯å¾„: {DATASET_PATH}")
    print(f"  â€¢ è®­ç»ƒæ£€æŸ¥ç‚¹ç›®å½•: {TRAINING_OUTPUT_DIR}")
    print(f"  â€¢ LoRAé€‚é…å™¨ä¿å­˜ç›®å½•: {LORA_ADAPTER_DIR}")
    print(f"  â€¢ åˆå¹¶æ¨¡å‹ä¿å­˜ç›®å½•: {MERGED_MODEL_DIR}")
    
    # æ­¥éª¤1: è·å–åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆè‡ªåŠ¨å¤„ç†ä¸‹è½½æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
    print("\nğŸ” ç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„...")
    if os.path.isdir(BASE_MODEL_SOURCE):
        print(f"  â€¢ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {BASE_MODEL_SOURCE}")
        base_model_path = BASE_MODEL_SOURCE
    else:
        print(f"  â€¢ ä» ModelScope ä¸‹è½½æ¨¡å‹: {BASE_MODEL_SOURCE}")
        base_model_path = snapshot_download(
            BASE_MODEL_SOURCE,
            cache_dir=MODEL_CACHE_DIR,
            revision="master"
        )
        print(f"  â€¢ æ¨¡å‹å·²ä¸‹è½½è‡³: {base_model_path}")
    
    # æ­¥éª¤2: åŠ è½½ tokenizer å’Œæ•°æ®é›†
    print("\nğŸ”„ åŠ è½½ tokenizer å’Œæ•°æ®é›†...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path, 
        use_fast=False, 
        trust_remote_code=True
    )
    
    tokenized_dataset = load_and_preprocess_data(DATASET_PATH, tokenizer)
    
    # æ­¥éª¤3: é…ç½® LoRA æ¨¡å‹
    print("\nğŸ”§ é…ç½® LoRA å¾®è°ƒæ¨¡å‹...")
    model = setup_lora_model(base_model_path)
    
    # æ­¥éª¤4: é…ç½®è®­ç»ƒ
    print("\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    trainer = setup_training(model, tokenizer, tokenized_dataset, TRAINING_OUTPUT_DIR)
    
    # æ­¥éª¤5: å¯åŠ¨è®­ç»ƒ
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # æ­¥éª¤6: ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜è®­ç»ƒç»“æœ...")
    
    # 6.1 ä¿å­˜ LoRA é€‚é…å™¨ (ä»…å¾®è°ƒå‚æ•°)
    print(f"  â€¢ ä¿å­˜ LoRA é€‚é…å™¨è‡³: {LORA_ADAPTER_DIR}")
    os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)
    model.save_pretrained(LORA_ADAPTER_DIR)
    tokenizer.save_pretrained(LORA_ADAPTER_DIR)
    print(f"    âœ… LoRA é€‚é…å™¨å·²ä¿å­˜ (çº¦ 10-20MB)")
    print(f"    ğŸ’¡ æç¤º: æ­¤ç›®å½•ä»…åŒ…å«å¾®è°ƒå‚æ•°ï¼Œæ¨ç†æ—¶éœ€ä¸åŸºç¡€æ¨¡å‹é…åˆä½¿ç”¨")
    
    # 6.2 ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
    print(f"  â€¢ åˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹è‡³: {MERGED_MODEL_DIR}")
    os.makedirs(MERGED_MODEL_DIR, exist_ok=True)
    
    # åˆå¹¶ LoRA é€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
    merged_model = model.merge_and_unload()
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹
    merged_model.save_pretrained(MERGED_MODEL_DIR)
    tokenizer.save_pretrained(MERGED_MODEL_DIR)
    print(f"    âœ… åˆå¹¶åçš„å®Œæ•´æ¨¡å‹å·²ä¿å­˜")
    print(f"    ğŸ’¡ æç¤º: æ­¤ç›®å½•åŒ…å«å®Œæ•´æ¨¡å‹ï¼Œå¯ç›´æ¥ç”¨äºæ¨ç†éƒ¨ç½²")
    
    print("\nğŸ‰ è®­ç»ƒå’Œä¿å­˜æµç¨‹å…¨éƒ¨å®Œæˆ!")


if __name__ == "__main__":
    main()