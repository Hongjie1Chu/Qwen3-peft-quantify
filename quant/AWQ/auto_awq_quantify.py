#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Qwen3-0.6B æ¨¡å‹é‡åŒ–è„šæœ¬ (AWQ)
åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. æ”¯æŒå®Œå…¨è‡ªå®šä¹‰è·¯å¾„é…ç½®
2. æ™ºèƒ½æ ¡å‡†æ•°æ®é›†å¤„ç†
3. æ¸…æ™°çš„é‡åŒ–æµç¨‹ç®¡ç†
"""

import os
import json
import random
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer


def load_calibration_data(calib_file, tokenizer, num_samples=10, seed=42):
    """åŠ è½½å¹¶å¤„ç†æ ¡å‡†æ•°æ®é›†"""
    print(f"ğŸ”„ åŠ è½½æ ¡å‡†æ•°æ®é›†: {calib_file}...")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(calib_file):
        raise FileNotFoundError(f"æ ¡å‡†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {calib_file}")
    
    # è¯»å–JSONLæ–‡ä»¶
    with open(calib_file, 'r') as f:
        lines = f.readlines()
    
    # è§£æJSONL
    data_list = [json.loads(line.strip()) for line in lines]
    
    # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    print(f"  â€¢ æ€»æ ·æœ¬æ•°: {len(data_list)}ï¼Œå°†éšæœºé€‰æ‹© {num_samples} æ¡ç”¨äºæ ¡å‡†")
    random.seed(seed)
    selected_data = random.sample(data_list, min(num_samples, len(data_list)))
    
    # è½¬æ¢ä¸ºQwenå¯¹è¯æ ¼å¼
    dataset = []
    for item in selected_data:
        messages = [
            {"role": "user", "content": item['input']},
            {"role": "assistant", "content": item['output']}
        ]
        dataset.append(messages)
    
    # åº”ç”¨tokenizerçš„chat template
    print("  â€¢ åº”ç”¨tokenizerçš„chat template...")
    data = []
    for msg in dataset:
        text = tokenizer.apply_chat_template(
            msg, 
            tokenize=False, 
            add_generation_prompt=False
        )
        data.append(text.strip())
    
    print(f"âœ… æ ¡å‡†æ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(data)} æ¡æœ‰æ•ˆæ•°æ®")
    return data


def quantize_model(model_path, quant_path, calib_data, quant_config):
    """æ‰§è¡Œæ¨¡å‹é‡åŒ–"""
    print(f"âš¡ å¼€å§‹æ¨¡å‹é‡åŒ– (è·¯å¾„: {model_path} â†’ {quant_path})...")
    
    # åŠ è½½æ¨¡å‹
    print(f"  â€¢ åŠ è½½æ¨¡å‹: {model_path}")
    model = AutoAWQForCausalLM.from_pretrained(
        model_path, 
        device_map="auto", 
        safetensors=True
    )
    
    # è·å–tokenizer
    print(f"  â€¢ åŠ è½½tokenizer")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # æ‰§è¡Œé‡åŒ–
    print("  â€¢ å¼€å§‹é‡åŒ–è¿‡ç¨‹...")
    print(f"  â€¢ é‡åŒ–é…ç½®: {quant_config}")
    
    model.quantize(
        tokenizer, 
        quant_config=quant_config,
        calib_data=calib_data,
        max_calib_seq_len=128  # æ ¡å‡†æ•°æ®å¤ªå°‘æ—¶ï¼Œè®¾ç½®å°ä¸€ç‚¹
    )
    
    # ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
    print(f"  â€¢ ä¿å­˜é‡åŒ–æ¨¡å‹è‡³: {quant_path}")
    os.makedirs(quant_path, exist_ok=True)
    model.save_quantized(quant_path, safetensors=True, shard_size="4GB")
    tokenizer.save_pretrained(quant_path)
    
    print(f"âœ… æ¨¡å‹é‡åŒ–å®Œæˆå¹¶ä¿å­˜è‡³: {quant_path}")
    return quant_path


def main():
    """ä¸»é‡åŒ–æµç¨‹ - æ‰€æœ‰è·¯å¾„é…ç½®åœ¨æ­¤å¤„è‡ªå®šä¹‰"""
    print("âœ¨ å¼€å§‹é…ç½®é‡åŒ–å‚æ•°...")
    
    # ======================
    # ç”¨æˆ·è‡ªå®šä¹‰è·¯å¾„é…ç½®åŒº
    # ======================
    
    # åŸºç¡€æ¨¡å‹
    BASE_MODEL_NAME = 'Qwen3-0.6B'
    # è¾“å‡ºç›®å½•é…ç½®
    OUTPUT_BASE_DIR = "../../output"
    
    # æ¨¡å‹é…ç½®
    MERGED_MODEL_PATH = f"{OUTPUT_BASE_DIR}/merged_model/{BASE_MODEL_NAME}/lora_merged_model"  # å¾…é‡åŒ–çš„åˆå¹¶æ¨¡å‹è·¯å¾„
    
    # æ•°æ®é›†é…ç½®
    CALIBRATION_DATASET = "../../datasets/peft_data.jsonl"  # æ ¡å‡†æ•°æ®é›†è·¯å¾„
    
    # é‡åŒ–è·¯å¾„
    QUANTIZED_MODEL_DIR = f"{OUTPUT_BASE_DIR}/quantized_model/{BASE_MODEL_NAME}-awq-4bit"
    
    # ======================
    # é‡åŒ–å‚æ•°é…ç½®åŒº
    # ======================
    
    QUANT_CONFIG = {
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"
    }
    
    CALIBRATION_SAMPLES = 10  # ç”¨äºæ ¡å‡†çš„æ ·æœ¬æ•°é‡
    RANDOM_SEED = 42          # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°
    
    # ======================
    # è·¯å¾„é…ç½®ç»“æŸ
    # ======================
    
    print("\nğŸ“Œ è‡ªå®šä¹‰é…ç½®:")
    print(f"  â€¢ å¾…é‡åŒ–æ¨¡å‹è·¯å¾„: {MERGED_MODEL_PATH}")
    print(f"  â€¢ æ ¡å‡†æ•°æ®é›†è·¯å¾„: {CALIBRATION_DATASET}")
    print(f"  â€¢ é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„: {QUANTIZED_MODEL_DIR}")
    print(f"  â€¢ é‡åŒ–é…ç½®: {QUANT_CONFIG}")
    print(f"  â€¢ æ ¡å‡†æ ·æœ¬æ•°é‡: {CALIBRATION_SAMPLES}")
    
    # æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    print("\nğŸ” éªŒè¯æ¨¡å‹è·¯å¾„...")
    if not os.path.exists(MERGED_MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MERGED_MODEL_PATH}")
    print(f"  â€¢ æ£€æµ‹åˆ°æ¨¡å‹è·¯å¾„: {MERGED_MODEL_PATH}")
    
    # æ­¥éª¤2: åŠ è½½tokenizerç”¨äºæ ¡å‡†æ•°æ®å¤„ç†
    print("\nğŸ”„ åŠ è½½tokenizerä»¥å¤„ç†æ ¡å‡†æ•°æ®...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)
    except Exception as e:
        raise RuntimeError(f"æ— æ³•åŠ è½½tokenizer: {str(e)}")
    
    # æ­¥éª¤3: åŠ è½½å¹¶å¤„ç†æ ¡å‡†æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ ¡å‡†æ•°æ®...")
    calib_data = load_calibration_data(
        CALIBRATION_DATASET, 
        tokenizer,
        num_samples=CALIBRATION_SAMPLES,
        seed=RANDOM_SEED
    )
    
    # æ­¥éª¤4: æ‰§è¡Œæ¨¡å‹é‡åŒ–
    print("\nâš¡ æ‰§è¡Œæ¨¡å‹é‡åŒ–...")
    quantize_model(
        model_path=MERGED_MODEL_PATH,
        quant_path=QUANTIZED_MODEL_DIR,
        calib_data=calib_data,
        quant_config=QUANT_CONFIG
    )
    
    print("\nğŸ‰ é‡åŒ–æµç¨‹å…¨éƒ¨å®Œæˆ!")


if __name__ == "__main__":
    # è®¾ç½®CUDAå¯è§è®¾å¤‡
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    main()