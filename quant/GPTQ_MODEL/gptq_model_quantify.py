#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Qwen3-0.6B GPTQé‡åŒ–è„šæœ¬
åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. ä¸é¡¹ç›®è·¯å¾„é…ç½®å®Œå…¨å…¼å®¹
2. ç®€åŒ–æ ¡å‡†æ•°æ®å¤„ç†æµç¨‹
3. æ¸…æ™°çš„é‡åŒ–è¿‡ç¨‹åé¦ˆ
"""

import os
import json
import torch
from gptqmodel import GPTQModel, QuantizeConfig
from transformers import AutoTokenizer


def load_calibration_data(calib_file, tokenizer, n_samples=100, seq_len=1024):
    """åŠ è½½å¹¶å¤„ç†æ ¡å‡†æ•°æ®é›†"""
    print(f"ğŸ”„ åŠ è½½æ ¡å‡†æ•°æ®é›†: {calib_file}...")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(calib_file):
        raise FileNotFoundError(f"æ ¡å‡†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {calib_file}")
    
    # è¯»å–JSONLæ–‡ä»¶
    with open(calib_file, 'r') as f:
        lines = f.readlines()
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    import random
    random.seed(42)
    selected_lines = random.sample(lines, min(n_samples, len(lines)))
    
    # å¤„ç†æ ¡å‡†æ•°æ®
    calibration_dataset = []
    skipped = 0
    
    for line in selected_lines:
        try:
            item = json.loads(line.strip())
            
            # æ„å»ºQwenå¯¹è¯æ ¼å¼
            messages = [
                {"role": "user", "content": item['input']},
                {"role": "assistant", "content": item['output']}
            ]
            
            # åº”ç”¨chat templateå¹¶ç¼–ç 
            text = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=False
            )
            
            # ç¼–ç å¹¶æˆªæ–­
            inputs = tokenizer(
                text, 
                max_length=seq_len, 
                truncation=True, 
                return_tensors="pt"
            )
            
            calibration_dataset.append({
                "input_ids": inputs["input_ids"][0],
                "attention_mask": inputs["attention_mask"][0]
            })
        except Exception as e:
            skipped += 1
            continue
    
    if skipped > 0:
        print(f"  âš ï¸ è·³è¿‡ {skipped} æ¡æ— æ•ˆæ•°æ®")
    
    print(f"âœ… æ ¡å‡†æ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(calibration_dataset)} æ¡æœ‰æ•ˆæ•°æ®")
    return calibration_dataset


def main():
    """ä¸»é‡åŒ–æµç¨‹"""
    print("âœ¨ å¼€å§‹é…ç½®é‡åŒ–å‚æ•°...")
    
    # ======================
    # ç”¨æˆ·è‡ªå®šä¹‰é…ç½®åŒº
    # ======================
    
    # åŸºç¡€æ¨¡å‹
    BASE_MODEL_NAME = 'Qwen3-0.6B'
    
    # è¾“å‡ºç›®å½•é…ç½®
    OUTPUT_BASE_DIR = "../../output"
    
    # æ¨¡å‹é…ç½®
    MERGED_MODEL_PATH = f"{OUTPUT_BASE_DIR}/merged_model/{BASE_MODEL_NAME}/lora_merged_model"
    
    # æ•°æ®é›†é…ç½®
    CALIBRATION_DATASET = "../../datasets/peft_data.jsonl"
    
    # é‡åŒ–è·¯å¾„
    QUANTIZED_MODEL_DIR = f"{OUTPUT_BASE_DIR}/quantized_model/{BASE_MODEL_NAME}-gptq-4bit"
    
    # ======================
    # é‡åŒ–å‚æ•°é…ç½®åŒº
    # ======================
    
    # é‡åŒ–é…ç½®
    QUANTIZE_CONFIG = QuantizeConfig(
        bits=4,                  # 4-bité‡åŒ–
        group_size=128,          # ä¸Qwen3çš„head_dimåŒ¹é…
        damp_percent=0.01,       # é˜»å°¼ç™¾åˆ†æ¯”
        desc_act=False,          # ç¦ç”¨desc_actä»¥æé«˜é€Ÿåº¦
        static_groups=False,     # ä¸ä½¿ç”¨é™æ€ç»„
        sym=True,                # å¯¹ç§°é‡åŒ–
        true_sequential=True,    # é¡ºåºé‡åŒ–
    )
    
    # æ ¡å‡†é…ç½®
    CALIBRATION_CONFIG = {
        "n_samples": 100,       # æ ¡å‡†æ ·æœ¬æ•°é‡
        "seq_len": 1024,        # åºåˆ—é•¿åº¦
    }
    
    # ======================
    # è·¯å¾„é…ç½®ç»“æŸ
    # ======================
    
    print("\nğŸ“Œ é…ç½®è¯¦æƒ…:")
    print(f"  â€¢ å¾…é‡åŒ–æ¨¡å‹è·¯å¾„: {MERGED_MODEL_PATH}")
    print(f"  â€¢ æ ¡å‡†æ•°æ®é›†è·¯å¾„: {CALIBRATION_DATASET}")
    print(f"  â€¢ é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„: {QUANTIZED_MODEL_DIR}")
    print(f"  â€¢ é‡åŒ–å‚æ•°: bits={QUANTIZE_CONFIG.bits}, group_size={QUANTIZE_CONFIG.group_size}")
    print(f"  â€¢ æ ¡å‡†å‚æ•°: æ ·æœ¬æ•°={CALIBRATION_CONFIG['n_samples']}, åºåˆ—é•¿åº¦={CALIBRATION_CONFIG['seq_len']}")
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    if not os.path.exists(MERGED_MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MERGED_MODEL_PATH}")
    
    # åŠ è½½tokenizer
    print("\nğŸ”„ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MERGED_MODEL_PATH, 
        use_fast=True, 
        trust_remote_code=True
    )
    
    # å¦‚æœæ²¡æœ‰pad_tokenï¼Œä½¿ç”¨eos_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½å¹¶å¤„ç†æ ¡å‡†æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ ¡å‡†æ•°æ®...")
    calibration_dataset = load_calibration_data(
        CALIBRATION_DATASET,
        tokenizer,
        n_samples=CALIBRATION_CONFIG["n_samples"],
        seq_len=CALIBRATION_CONFIG["seq_len"]
    )
    
    if len(calibration_dataset) == 0:
        raise ValueError("æ ¡å‡†æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œè·¯å¾„")
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ”§ åŠ è½½æ¨¡å‹...")
    model = GPTQModel.from_pretrained(
        MERGED_MODEL_PATH,
        quantize_config=QUANTIZE_CONFIG,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    # æ‰§è¡Œé‡åŒ–
    print("\nâš¡ æ‰§è¡Œé‡åŒ–è¿‡ç¨‹...")
    model.quantize(calibration_dataset)
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹...")
    os.makedirs(QUANTIZED_MODEL_DIR, exist_ok=True)
    model.save_quantized(QUANTIZED_MODEL_DIR)
    tokenizer.save_pretrained(QUANTIZED_MODEL_DIR)
    
    print(f"\nğŸ‰ é‡åŒ–å®Œæˆ! æ¨¡å‹å·²ä¿å­˜è‡³: {QUANTIZED_MODEL_DIR}")
    
if __name__ == "__main__":
    main()